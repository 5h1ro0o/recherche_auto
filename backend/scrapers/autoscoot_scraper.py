from typing import List, Dict, Any, Optional
import logging
import re
from .base_scraper import BaseScraper

logger = logging.getLogger(__name__)

class AutoScout24Scraper(BaseScraper):
    """Scraper pour AutoScout24 avec extraction compl√®te des donn√©es"""

    BASE_URL = "https://www.autoscout24.fr"

    def get_source_name(self) -> str:
        return "autoscout24"

    def scrape(self, search_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Scrape AutoScout24 avec support des filtres

        search_params:
            - max_pages: int (d√©faut: 3)
            - make: str (optionnel, ex: 'volkswagen')
            - model: str (optionnel, ex: 'golf')
            - min_year: int (optionnel)
            - max_year: int (optionnel)
            - max_price: int (optionnel)
            - fuel_type: str (optionnel: 'B' benzine, 'D' diesel, 'E' √©lectrique)
        """
        max_pages = search_params.get('max_pages', 3)
        results = []

        try:
            self.init_browser(headless=True)
            logger.info(f"üîµ AutoScout24: Scraping {max_pages} pages")

            # Construire l'URL de recherche
            base_search_url = self._build_search_url(search_params)

            # Navigation initiale
            logger.info(f"üîó URL: {base_search_url}")
            self.page.goto(base_search_url, wait_until='domcontentloaded', timeout=30000)
            self.random_delay(3, 5)

            # G√©rer les cookies si pr√©sents
            self._handle_cookie_banner()

            for page_num in range(max_pages):
                logger.info(f"üìÑ Page {page_num + 1}/{max_pages}")

                if page_num > 0:
                    # Naviguer vers la page suivante
                    page_url = f"{base_search_url}&page={page_num + 1}"
                    self.page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                    self.random_delay(2, 4)

                # Attendre les r√©sultats
                page_results = self._scrape_page()

                if not page_results:
                    logger.warning(f"‚ö†Ô∏è Aucun r√©sultat page {page_num + 1}, arr√™t")
                    break

                results.extend(page_results)
                logger.info(f"‚úÖ Page {page_num + 1}: {len(page_results)} annonces")

                # D√©lai entre pages
                self.random_delay(2, 4)

            logger.info(f"üéâ AutoScout24 termin√©: {len(results)} annonces r√©cup√©r√©es")

        except Exception as e:
            logger.exception(f"‚ùå Erreur AutoScout24: {e}")
        finally:
            self.close_browser()

        return results

    def _build_search_url(self, params: Dict[str, Any]) -> str:
        """Construit l'URL de recherche avec filtres"""
        url_parts = [f"{self.BASE_URL}/lst"]
        query_params = []

        # Tri par d√©faut
        query_params.append("sort=standard")
        query_params.append("desc=0")

        # Type de v√©hicule (voiture)
        query_params.append("atype=C")

        # √âtat (neuf et occasion)
        query_params.append("ustate=N,U")

        # Nombre de r√©sultats par page
        query_params.append("size=20")

        # Marque
        if params.get('make'):
            make = params['make'].lower()
            query_params.append(f"mmvmk0={make}")

        # Mod√®le
        if params.get('model'):
            model = params['model'].lower()
            query_params.append(f"mmvmd0={model}")

        # Ann√©e
        if params.get('min_year'):
            query_params.append(f"fregfrom={params['min_year']}")
        if params.get('max_year'):
            query_params.append(f"fregto={params['max_year']}")

        # Prix
        if params.get('max_price'):
            query_params.append(f"priceto={params['max_price']}")

        # Carburant
        if params.get('fuel_type'):
            query_params.append(f"fuel={params['fuel_type']}")

        return f"{url_parts[0]}?{'&'.join(query_params)}"

    def _handle_cookie_banner(self):
        """Accepte les cookies si le banner est pr√©sent"""
        try:
            # Chercher le bouton d'acceptation des cookies
            accept_button = self.page.query_selector('button[data-testid="accept-all-button"]')
            if not accept_button:
                accept_button = self.page.query_selector('button:has-text("Accepter")')
            if not accept_button:
                accept_button = self.page.query_selector('button:has-text("Accept")')

            if accept_button:
                accept_button.click()
                self.random_delay(1, 2)
                logger.info("‚úÖ Cookies accept√©s")
        except Exception as e:
            logger.debug(f"Pas de banner cookies ou erreur: {e}")

    def _scrape_page(self) -> List[Dict[str, Any]]:
        """Scrape une page de r√©sultats"""
        results = []

        try:
            # Attendre que les annonces se chargent avec plusieurs s√©lecteurs possibles
            selectors = [
                'article[data-testid="listing-item"]',  # S√©lecteur sp√©cifique
                'article.ListItem_article',  # Avec classe
                'article',  # G√©n√©rique - tous les articles
                'div[data-testid="search-listing"]',  # Container de recherche
                'a[href*="/annonces/"]',  # Liens vers annonces
                'div[class*="ListItem"]'  # Contient ListItem dans la classe
            ]

            listings = []
            for selector in selectors:
                try:
                    self.page.wait_for_selector(selector, timeout=10000)
                    elements = self.page.query_selector_all(selector)

                    # Filtrer pour ne garder que les √©l√©ments pertinents
                    if selector == 'article':
                        # Pour le s√©lecteur g√©n√©rique, v√©rifier qu'il y a un lien d'annonce
                        listings = [el for el in elements if el.query_selector('a[href*="/annonces/"]')]
                    else:
                        listings = elements

                    if listings and len(listings) > 0:
                        logger.info(f"‚úÖ Trouv√© {len(listings)} annonces avec s√©lecteur: {selector}")
                        break
                except Exception as e:
                    logger.debug(f"S√©lecteur '{selector}' non trouv√©: {e}")
                    continue

            if not listings or len(listings) == 0:
                logger.warning("‚ö†Ô∏è Aucune annonce trouv√©e avec tous les s√©lecteurs")
                return results

            for idx, listing in enumerate(listings):
                try:
                    parsed = self.parse_listing(listing)
                    if parsed:
                        normalized = self.normalize_data(parsed)
                        results.append(normalized)
                        logger.debug(f"‚úì Annonce {idx+1}: {parsed.get('title', 'N/A')[:50]}")
                    else:
                        logger.debug(f"‚úó Annonce {idx+1}: Parsing retourn√© None")
                except Exception as e:
                    logger.warning(f"‚úó Annonce {idx+1}: {e}")

            logger.info(f"üìä Pars√© {len(results)}/{len(listings)} annonces avec succ√®s")

        except Exception as e:
            logger.error(f"Erreur scraping page: {e}")

        return results

    def parse_listing(self, element) -> Optional[Dict[str, Any]]:
        """Parse une annonce AutoScout24 avec extraction g√©n√©rique et robuste"""
        try:
            # Trouver le lien principal (plusieurs m√©thodes)
            link = None
            if element.tag_name == 'a':
                link = element
            else:
                # Chercher le premier lien dans l'√©l√©ment
                link = element.query_selector('a[href*="/annonces/"]') or element.query_selector('a')

            if not link:
                return None

            url = link.get_attribute('href')
            if not url or '/annonces/' not in url:
                return None

            # Construire l'URL compl√®te
            if url.startswith('/'):
                url = f"{self.BASE_URL}{url}"

            # Extraire l'ID depuis l'URL
            source_id = url.split('/')[-1] if '/' in url else 'unknown'

            # Extraction G√âN√âRIQUE du texte complet de l'√©l√©ment
            full_text = element.inner_text() if element else ""

            # Titre - essayer plusieurs s√©lecteurs puis fallback sur premier h2/h3
            title = None
            title_selectors = ['h2', 'h3', '[data-testid*="title"]', '[class*="title"]', '[class*="Title"]', 'a']
            for selector in title_selectors:
                title_elem = element.query_selector(selector)
                if title_elem:
                    title = title_elem.inner_text().strip()
                    if title and len(title) > 3:  # Au moins 3 caract√®res
                        break

            # Si pas de titre, essayer d'extraire de l'attribut du lien
            if not title and link:
                title = link.get_attribute('title') or link.get_attribute('aria-label')

            # Prix - chercher pattern de prix dans le texte
            price = None
            price_patterns = [
                r'(\d{1,3}(?:\s?\d{3})*)\s*‚Ç¨',  # Ex: 15 000 ‚Ç¨
                r'‚Ç¨\s*(\d{1,3}(?:\s?\d{3})*)',  # Ex: ‚Ç¨ 15000
            ]
            for pattern in price_patterns:
                match = re.search(pattern, full_text)
                if match:
                    price = match.group(1).replace(' ', '')
                    break

            # Ann√©e - chercher pattern 4 chiffres entre 1990 et 2030
            year = None
            year_match = re.search(r'\b(19\d{2}|20[0-3]\d)\b', full_text)
            if year_match:
                year = int(year_match.group(1))

            # Kilom√©trage - chercher pattern de km
            mileage = None
            km_patterns = [
                r'(\d{1,3}(?:\s?\d{3})*)\s*km',
                r'(\d{1,3}(?:\s?\d{3})*)\s*Kilom√®tres',
            ]
            for pattern in km_patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    mileage = match.group(1).replace(' ', '')
                    break

            # Carburant - chercher mots-cl√©s
            fuel_type = None
            fuel_keywords = ['Diesel', 'Essence', '√âlectrique', 'Hybride', 'GPL']
            for keyword in fuel_keywords:
                if keyword.lower() in full_text.lower():
                    fuel_type = keyword
                    break

            # Transmission
            transmission = None
            if 'automatique' in full_text.lower():
                transmission = 'Automatique'
            elif 'manuelle' in full_text.lower():
                transmission = 'Manuelle'

            # Image
            images = []
            img_elem = element.query_selector('img')
            if img_elem:
                img_src = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
                if img_src and not img_src.endswith('.svg') and 'placeholder' not in img_src:
                    images.append(img_src)

            # Extraire marque et mod√®le du titre
            make = None
            model = None
            if title:
                parts = title.split()
                if len(parts) >= 1:
                    make = parts[0]
                if len(parts) >= 2:
                    model = ' '.join(parts[1:3])  # 2 premiers mots apr√®s la marque

            # Retourner au moins avec URL et titre
            if not title or len(title) < 3:
                return None  # Pas assez d'infos

            return {
                'id': source_id,
                'title': title,
                'make': make,
                'model': model,
                'price': price,
                'year': year,
                'mileage': mileage,
                'fuel_type': fuel_type,
                'transmission': transmission,
                'location': None,  # Pas extrait pour l'instant
                'url': url,
                'images': images
            }

        except Exception as e:
            logger.warning(f"Erreur parse listing: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None