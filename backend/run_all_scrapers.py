#!/usr/bin/env python3
"""
Script principal pour lancer TOUS les scrapers de l'encyclop√©die automobile
Collecte automatiquement TOUTES les donn√©es depuis Internet :
- Mod√®les de voitures (specs, avis, caract√©ristiques)
- Moteurs (specs techniques, fiabilit√©, probl√®mes communs)
- Transmissions (fiabilit√©, retours utilisateurs)
- Marques (historique, r√©putation)
"""

import asyncio
import sys
import os
from datetime import datetime

# Importer les scrapers
try:
    from scrape_models_web import AutoWebScraper
    from scrape_engines_web import EngineWebScraper
    from scrape_transmissions_web import TransmissionWebScraper
except ImportError as e:
    print(f"‚ùå Erreur import des scrapers: {e}")
    print("Assurez-vous que tous les scripts sont dans le m√™me r√©pertoire")
    sys.exit(1)


class MasterScraper:
    """Orchestrateur principal de tous les scrapers"""

    def __init__(self):
        self.start_time = None
        self.stats = {
            'models': 0,
            'engines': 0,
            'transmissions': 0,
            'errors': 0,
        }

    def print_banner(self):
        """Affiche la banni√®re de d√©marrage"""
        print("\n")
        print("=" * 100)
        print("‚ïë" + " " * 98 + "‚ïë")
        print("‚ïë" + "SCRAPING AUTOMATIQUE ENCYCLOP√âDIE AUTOMOBILE".center(98) + "‚ïë")
        print("‚ïë" + " " * 98 + "‚ïë")
        print("‚ïë" + "Collecte TOUTES les donn√©es depuis Internet".center(98) + "‚ïë")
        print("‚ïë" + " " * 98 + "‚ïë")
        print("=" * 100)
        print("\n")

    def print_section(self, title: str):
        """Affiche un titre de section"""
        print("\n")
        print("‚îå" + "‚îÄ" * 98 + "‚îê")
        print("‚îÇ" + title.center(98) + "‚îÇ")
        print("‚îî" + "‚îÄ" * 98 + "‚îò")
        print("\n")

    async def scrape_all_models(self):
        """Lance le scraping de tous les mod√®les"""
        self.print_section("üöó SCRAPING DES MOD√àLES AUTOMOBILES")

        scraper = AutoWebScraper()
        await scraper.init_session()

        try:
            # R√©cup√©rer les marques depuis la DB
            from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
            from sqlalchemy.orm import sessionmaker
            from dotenv import load_dotenv

            load_dotenv()
            DATABASE_URL = os.getenv("DATABASE_URL", "").replace("postgresql://", "postgresql+asyncpg://")

            engine = create_async_engine(DATABASE_URL, echo=False)
            async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

            async with async_session() as session:
                result = await session.execute("SELECT id, name FROM car_brands")
                brands = result.fetchall()

                print(f"üìã {len(brands)} marques trouv√©es")
                print("üåê Sources : CarQuery API, Automobile-Catalog, Caradisiac, L'Argus")
                print("\n")

                total_models = []

                for brand_id, brand_name in brands:
                    print(f"\n{'‚îÄ' * 80}")
                    print(f"Marque: {brand_name}")
                    print(f"{'‚îÄ' * 80}")

                    models = await scraper.collect_all_models_for_brand(brand_name, brand_id)
                    total_models.extend(models)

                    print(f"‚úÖ {brand_name}: {len(models)} mod√®les collect√©s")

                    # Sauvegarder par batch
                    if len(total_models) >= 50:
                        await scraper.save_models_to_db(total_models)
                        self.stats['models'] += len(total_models)
                        total_models = []

                # Sauvegarder le reste
                if total_models:
                    await scraper.save_models_to_db(total_models)
                    self.stats['models'] += len(total_models)

                print(f"\n‚úÖ TOTAL MOD√àLES: {self.stats['models']} collect√©s et sauvegard√©s")

        except Exception as e:
            print(f"‚ùå Erreur scraping mod√®les: {e}")
            self.stats['errors'] += 1
        finally:
            await scraper.close_session()

    async def scrape_all_engines(self):
        """Lance le scraping de tous les moteurs"""
        self.print_section("üîß SCRAPING DES MOTEURS AUTOMOBILES")

        scraper = EngineWebScraper()
        await scraper.init_session()

        try:
            manufacturers = [
                'Renault', 'PSA', 'Volkswagen', 'BMW', 'Mercedes-Benz',
                'Audi', 'Toyota', 'Ford', 'Honda', 'Nissan', 'Hyundai', 'Kia'
            ]

            print(f"üìã {len(manufacturers)} constructeurs")
            print("üåê Sources : Automobile-Catalog, Caradisiac, L'Argus, Forums")
            print("\n")

            engines = await scraper.collect_all_engines_data(manufacturers)

            if engines:
                await scraper.save_engines_to_db(engines)
                self.stats['engines'] = len(engines)
                print(f"\n‚úÖ TOTAL MOTEURS: {self.stats['engines']} collect√©s et sauvegard√©s")

        except Exception as e:
            print(f"‚ùå Erreur scraping moteurs: {e}")
            self.stats['errors'] += 1
        finally:
            await scraper.close_session()

    async def scrape_all_transmissions(self):
        """Lance le scraping de toutes les transmissions"""
        self.print_section("‚öôÔ∏è  SCRAPING DES TRANSMISSIONS AUTOMOBILES")

        scraper = TransmissionWebScraper()
        await scraper.init_session()

        try:
            print("üåê Sources : Caradisiac, L'Argus, Forums automobiles")
            print("\n")

            transmissions = await scraper.collect_all_transmissions_data()

            if transmissions:
                await scraper.save_transmissions_to_db(transmissions)
                self.stats['transmissions'] = len(transmissions)
                print(f"\n‚úÖ TOTAL TRANSMISSIONS: {self.stats['transmissions']} collect√©es et sauvegard√©es")

        except Exception as e:
            print(f"‚ùå Erreur scraping transmissions: {e}")
            self.stats['errors'] += 1
        finally:
            await scraper.close_session()

    def print_final_stats(self):
        """Affiche les statistiques finales"""
        duration = datetime.now() - self.start_time
        hours, remainder = divmod(int(duration.total_seconds()), 3600)
        minutes, seconds = divmod(remainder, 60)

        print("\n")
        print("=" * 100)
        print("‚ïë" + " " * 98 + "‚ïë")
        print("‚ïë" + "STATISTIQUES FINALES".center(98) + "‚ïë")
        print("‚ïë" + " " * 98 + "‚ïë")
        print("=" * 100)
        print(f"\nüìä Mod√®les collect√©s      : {self.stats['models']:>6}")
        print(f"üìä Moteurs collect√©s      : {self.stats['engines']:>6}")
        print(f"üìä Transmissions collect√©es: {self.stats['transmissions']:>6}")
        print(f"{'‚îÄ' * 50}")
        print(f"üìä TOTAL                  : {sum([self.stats['models'], self.stats['engines'], self.stats['transmissions']]):>6}")
        print(f"\n‚ö†Ô∏è  Erreurs               : {self.stats['errors']:>6}")
        print(f"\n‚è±Ô∏è  Dur√©e totale          : {hours:02d}h {minutes:02d}m {seconds:02d}s")
        print("\n" + "=" * 100)

        if self.stats['errors'] == 0:
            print("\n‚úÖ Scraping termin√© avec succ√®s !")
        else:
            print(f"\n‚ö†Ô∏è  Scraping termin√© avec {self.stats['errors']} erreur(s)")

        print("\n")

    async def run(self):
        """Lance tous les scrapers s√©quentiellement"""
        self.start_time = datetime.now()
        self.print_banner()

        print(f"üïê D√©marrage: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")

        # 1. Scraper les mod√®les
        await self.scrape_all_models()

        # 2. Scraper les moteurs
        await self.scrape_all_engines()

        # 3. Scraper les transmissions
        await self.scrape_all_transmissions()

        # Afficher les stats finales
        self.print_final_stats()


async def main():
    """Point d'entr√©e principal"""
    master = MasterScraper()

    try:
        await master.run()
        return 0
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Scraping interrompu par l'utilisateur")
        master.print_final_stats()
        return 1
    except Exception as e:
        print(f"\n\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
